/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import OpenAI from 'openai';
import {
  ContentGenerator,
  ContentGeneratorConfig,
} from '../core/contentGenerator.js';
import {
  GenerateContentParameters,
  GenerateContentResponse,
  CountTokensParameters,
  CountTokensResponse,
  EmbedContentParameters,
  EmbedContentResponse,
  Content,
  Part,
  Tool,
  FunctionCall,
  FunctionResponse,
} from '@google/genai';

export class OpenAIAdapter implements ContentGenerator {
  private openai: OpenAI;
  private model: string;

  constructor(config: ContentGeneratorConfig) {
    this.openai = new OpenAI({
      apiKey: config.apiKey,
      baseURL: config.baseURL,
    });
    this.model = config.model;
  }

  async generateContent(
    request: GenerateContentParameters,
  ): Promise<GenerateContentResponse> {
    try {
      const messages = this.convertGeminiToOpenAIMessages(request.contents);
      const tools = this.convertGeminiToOpenAITools(request.tools);

      const openaiRequest: OpenAI.Chat.ChatCompletionCreateParams = {
        model: this.model,
        messages,
        max_tokens: request.generationConfig?.maxOutputTokens,
        temperature: request.generationConfig?.temperature,
        top_p: request.generationConfig?.topP,
        ...(tools.length > 0 && { tools, tool_choice: 'auto' }),
      };

      const response = await this.openai.chat.completions.create(openaiRequest);
      return this.convertOpenAIToGeminiResponse(response);
    } catch (error) {
      throw new Error(`OpenAI API error: ${error}`);
    }
  }

  async *generateContentStream(
    request: GenerateContentParameters,
  ): AsyncGenerator<GenerateContentResponse> {
    try {
      const messages = this.convertGeminiToOpenAIMessages(request.contents);
      const tools = this.convertGeminiToOpenAITools(request.tools);

      const openaiRequest: OpenAI.Chat.ChatCompletionCreateParams = {
        model: this.model,
        messages,
        stream: true,
        max_tokens: request.generationConfig?.maxOutputTokens,
        temperature: request.generationConfig?.temperature,
        top_p: request.generationConfig?.topP,
        ...(tools.length > 0 && { tools, tool_choice: 'auto' }),
      };

      const stream = await this.openai.chat.completions.create(openaiRequest);
      
      for await (const chunk of stream) {
        const geminiResponse = this.convertOpenAIStreamToGeminiResponse(chunk);
        if (geminiResponse) {
          yield geminiResponse;
        }
      }
    } catch (error) {
      throw new Error(`OpenAI API streaming error: ${error}`);
    }
  }

  async countTokens(request: CountTokensParameters): Promise<CountTokensResponse> {
    // OpenAI doesn't have a direct token counting API, so we estimate
    // This is a simplified estimation - in production you might want to use tiktoken
    const text = request.contents?.map(content => 
      content.parts?.map(part => part.text || '').join(' ') || ''
    ).join(' ') || '';
    
    // Rough estimation: ~4 characters per token for English text
    const estimatedTokens = Math.ceil(text.length / 4);
    
    return {
      totalTokens: estimatedTokens,
    };
  }

  async embedContent(request: EmbedContentParameters): Promise<EmbedContentResponse> {
    try {
      const text = request.content?.parts?.[0]?.text || '';
      
      const response = await this.openai.embeddings.create({
        model: 'text-embedding-ada-002', // Default OpenAI embedding model
        input: text,
      });

      return {
        embedding: {
          values: response.data[0].embedding,
        },
      };
    } catch (error) {
      throw new Error(`OpenAI embedding error: ${error}`);
    }
  }

  private convertGeminiToOpenAIMessages(contents?: Content[]): OpenAI.Chat.ChatCompletionMessageParam[] {
    if (!contents) return [];

    return contents.map(content => {
      const text = content.parts?.filter(part => part.text).map(part => part.text).join('') || '';
      const functionCalls = content.parts?.filter(part => part.functionCall);
      const functionResponses = content.parts?.filter(part => part.functionResponse);

      if (functionCalls?.length) {
        return {
          role: 'assistant' as const,
          content: text || null,
          tool_calls: functionCalls.map(part => ({
            id: `call_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
            type: 'function' as const,
            function: {
              name: part.functionCall?.name || '',
              arguments: JSON.stringify(part.functionCall?.args || {}),
            },
          })),
        };
      }

      if (functionResponses?.length) {
        return {
          role: 'tool' as const,
          content: JSON.stringify(functionResponses[0].functionResponse?.response || {}),
          tool_call_id: `call_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
        };
      }

      return {
        role: content.role === 'model' ? 'assistant' as const : content.role as 'user' | 'system',
        content: text,
      };
    });
  }

  private convertGeminiToOpenAITools(tools?: Tool[]): OpenAI.Chat.ChatCompletionTool[] {
    if (!tools) return [];

    return tools.flatMap(tool => 
      tool.functionDeclarations?.map(func => ({
        type: 'function' as const,
        function: {
          name: func.name,
          description: func.description,
          parameters: func.parameters,
        },
      })) || []
    );
  }

  private convertOpenAIToGeminiResponse(response: OpenAI.Chat.ChatCompletion): GenerateContentResponse {
    const choice = response.choices[0];
    const message = choice.message;

    const parts: Part[] = [];

    if (message.content) {
      parts.push({ text: message.content });
    }

    if (message.tool_calls) {
      message.tool_calls.forEach(toolCall => {
        parts.push({
          functionCall: {
            name: toolCall.function.name,
            args: JSON.parse(toolCall.function.arguments || '{}'),
          },
        });
      });
    }

    return {
      candidates: [{
        content: {
          role: 'model',
          parts,
        },
        finishReason: this.mapOpenAIFinishReason(choice.finish_reason),
      }],
      usageMetadata: response.usage ? {
        promptTokenCount: response.usage.prompt_tokens,
        candidatesTokenCount: response.usage.completion_tokens,
        totalTokenCount: response.usage.total_tokens,
      } : undefined,
    };
  }

  private convertOpenAIStreamToGeminiResponse(chunk: OpenAI.Chat.Completions.ChatCompletionChunk): GenerateContentResponse | null {
    const choice = chunk.choices[0];
    if (!choice) return null;

    const delta = choice.delta;
    const parts: Part[] = [];

    if (delta.content) {
      parts.push({ text: delta.content });
    }

    if (delta.tool_calls) {
      delta.tool_calls.forEach(toolCall => {
        if (toolCall.function?.name) {
          parts.push({
            functionCall: {
              name: toolCall.function.name,
              args: JSON.parse(toolCall.function.arguments || '{}'),
            },
          });
        }
      });
    }

    if (parts.length === 0) return null;

    return {
      candidates: [{
        content: {
          role: 'model',
          parts,
        },
        finishReason: choice.finish_reason ? this.mapOpenAIFinishReason(choice.finish_reason) : undefined,
      }],
    };
  }

  private mapOpenAIFinishReason(reason: string | null): string {
    switch (reason) {
      case 'stop':
        return 'STOP';
      case 'length':
        return 'MAX_TOKENS';
      case 'function_call':
      case 'tool_calls':
        return 'STOP';
      case 'content_filter':
        return 'SAFETY';
      default:
        return 'OTHER';
    }
  }
}
